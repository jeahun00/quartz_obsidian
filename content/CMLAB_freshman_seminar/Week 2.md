# Sung Kim 모두의 딥러닝

### Lec 8 : 딥러닝의 기본 개념과, 문제, 그리고 해결
- [x] Lec ✅ 2023-11-12
- [x] Lab ✅ 2023-11-12
- [x] ppt 정리 ✅ 2023-11-12

### Lec 9 : Neural Network 1: XOR 문제와 학습방법, Backpropagation (1986 breakthrough)
- [x] Lec ✅ 2023-11-13
- [x] Lab ✅ 2023-11-13
- [x] ppt 정리 ✅ 2023-11-13

### Lec 10 : Neural Network 2: ReLU and 초기값 정하기 (2006/2007 breakthrough)
- [x] Lec ✅ 2023-11-14
- [x] Lab ✅ 2023-11-14
- [x] ppt 정리 ✅ 2023-11-17
Q1. Gradient Descent는 Back Propagation의 Weight Update 를 위한 기법이라고 설명하면 맞는 말인가?(GD 와 BP Weight Update 는 다른 개념인가?)
Q2. 영상에서 ReLU 를 쓰는 이유가 Sigmoid 는 값이 0과 1사이로 한정되어서라고 했는데 그게 아니라 미분했을 때 sigmoid의 최대값은 0.3 근방이므로 레이어가 쌓일수록 Gradient 가 작아지는 것이 맞는 설명 아닌가?

### Lec 11 : Convolutional Neural Networks
- [x] Lec ✅ 2023-11-17
- [x] Lab ✅ 2023-11-17
- [x] ppt 정리 ✅ 2023-11-17
### Lec 12 : Recurrent Neural Network
- [x] Lec ✅ 2023-11-17
- [x] Lab ✅ 2023-11-17
- [x] ppt 정리 ✅ 2023-11-17