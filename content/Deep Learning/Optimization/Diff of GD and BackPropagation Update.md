#Deep_Learning 

# Q : Gradient Descent 와 Back Propagation 에서의 Weight Update 는 동일한 개념인가?

$$

$$

1. **경사 하강법 (Gradient Descent)**:
    
    - **목적**: 경사 하강법은 손실 함수를 최소화하기 위해 모델의 매개변수(예: 가중치)를 조정하는 최적화 알고리즘입니다.
    - **작동 방식**: 이 방법은 손실 함수의 그래디언트(기울기)를 계산하고, 이 그래디언트를 사용하여 매개변수를 업데이트합니다. 기본적으로, 그래디언트는 손실을 가장 빠르게 감소시키는 방향을 나타냅니다.
    - **역할**: 손실 함수의 값에 대한 매개변수의 편미분을 사용하여 매개변수를 반복적으로 조정합니다.
2. **역전파 (Back Propagation)**:
    
    - **목적**: 역전파는 신경망에서 경사 하강법을 적용하기 위해 필요한 그래디언트를 효율적으로 계산하는 알고리즘입니다.
    - **작동 방식**: 신경망의 출력에서 발생하는 오차를 계산하고, 이 오차를 신경망을 거슬러 올라가면서 각 레이어의 가중치에 대한 오차의 영향을 계산합니다.
    - **역할**: 각 레이어에서의 가중치에 대한 손실 함수의 그래디언트를 계산하며, 이를 통해 경사 하강법에 필요한 정보를 제공합니다.

**차이점과 관계**:

- **차이점**: 경사 하강법은 최적화의 일반적인 방법이며, 손실 함수의 최소화를 목표로 하는 방식입니다. 반면, 역전파는 신경망에서 그래디언트를 계산하는 구체적인 메커니즘입니다.
- **관계**: 역전파는 경사 하강법을 신경망에 적용하기 위해 필요한 그래디언트를 계산하는 과정입니다. 즉, 역전파는 경사 하강법을 실행하기 위한 필수적인 단계입니다.

-> 위의 설명과 같이 Back Propagation 에서 작업하는 Weight update 를 Gradient Descent 를 이용하여 연산한다. 