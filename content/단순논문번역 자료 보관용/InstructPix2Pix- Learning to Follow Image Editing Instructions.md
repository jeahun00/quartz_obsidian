# Abstract

우리는 사람의 지시에 따라 이미지를 편집하는 방법을 제안합니다: 입력 이미지와 모델에게 무엇을 해야 할지 알려주는 서면 지시를 주면, 우리의 모델은 이 지시를 따라 이미지를 편집합니다. 이 문제에 대한 훈련 데이터를 얻기 위해, 두 개의 큰 사전 훈련된 모델인 언어 모델(GPT-3)과 텍스트-이미지 모델(Stable Diffusion)의 지식을 결합하여 대규모 이미지 편집 예시 데이터셋을 생성합니다. 우리의 조건부 diffusion 모델인 InstructPix2Pix은 생성된 데이터에 훈련되어 실제 이미지와 사용자가 작성한 지시에 대해서도 일반화되며, 추론 시간에 이미지를 편집합니다. 우리의 모델은 전방향 패스에서 편집을 수행하며, 예시별로 미세 조정이나 inversion을 요구하지 않기 때문에 몇 초 내에 이미지를 빠르게 편집합니다. 다양한 입력 이미지와 서면 지시에 대해 설득력 있는 편집 결과를 보여줍니다.


# 1. Introduction

우리는 이미지 편집을 위한 인간이 작성한 지시를 따를 수 있는 생성 모델을 교육하는 방법을 제시합니다. 이 작업을 위한 훈련 데이터를 대규모로 확보하는 것은 어렵기 때문에, 다양한 모달리티에서 사전 훈련된 여러 대형 모델을 결합한 쌍 데이터셋을 생성하는 접근 방식을 제안합니다: 대형 언어 모델(GPT-3 [7])과 텍스트-이미지 모델(Stable Diffusion [51]). 이 두 모델은 언어와 이미지에 대한 보완적인 지식을 포착하여 두 모달리티를 아우르는 작업을 위한 쌍 훈련 데이터를 생성하는 데 결합될 수 있습니다.

우리가 생성한 쌍 데이터를 사용하여, 입력 이미지와 그것을 편집하는 방법에 대한 텍스트 지시가 주어졌을 때 편집된 이미지를 생성하는 조건부 diffusion 모델을 훈련합니다. 우리의 모델은 전방향 패스에서 직접 이미지 편집을 수행하며, 추가적인 예시 이미지, 입력/출력 이미지의 전체 설명, 또는 예시별 미세 조정을 요구하지 않습니다. 전적으로 합성 예시(즉, 생성된 서면 지시와 생성된 이미지)에 대해 훈련되었음에도 불구하고, 우리의 모델은 임의의 실제 이미지와 자연스러운 인간 작성 지시에 대해 제로-샷 일반화를 달성합니다. 우리의 모델은 객체 교체, 이미지의 스타일 변경, 설정 변경, 예술 매체 변경 등 다양한 편집을 수행할 수 있는 인간의 지시를 따를 수 있는 직관적인 이미지 편집을 가능하게 합니다. 선택된 예시는 그림 1에서 찾을 수 있습니다.

# 2. Prior Work
대형 사전 훈련된 모델의 조합
최근 연구에서는 대형 사전 훈련된 모델들을 조합하여 단일 모델로는 수행할 수 없는 다모달 작업, 예를 들어 이미지 캡셔닝과 시각적 질문 답변(언어 모델과 텍스트-이미지 모델 모두의 지식이 요구되는 작업)을 해결할 수 있음을 보여주었습니다. 사전 훈련된 모델들을 결합하는 기법에는 새로운 작업에 대한 공동 미세조정[4, 33, 40, 67], 프롬프트를 통한 의사소통[62, 69], 에너지 기반 모델의 확률 분포 조합[11, 37], 다른 모델로부터의 피드백으로 하나의 모델을 안내하는 방법[61], 그리고 반복적 최적화[34]가 있습니다. 우리의 방법은 GPT-3 [7]과 Stable Diffusion [51]이라는 두 개의 사전 훈련된 모델의 보완적 능력을 활용한다는 점에서 이전 작업과 유사하지만, 이 모델들을 사용하여 쌍을 이루는 다모달 훈련 데이터를 생성한다는 점에서 차이가 있습니다.