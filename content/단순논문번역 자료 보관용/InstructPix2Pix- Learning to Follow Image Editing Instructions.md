# Abstract

우리는 사람의 지시에 따라 이미지를 편집하는 방법을 제안합니다: 입력 이미지와 모델에게 무엇을 해야 할지 알려주는 서면 지시를 주면, 우리의 모델은 이 지시를 따라 이미지를 편집합니다. 이 문제에 대한 훈련 데이터를 얻기 위해, 두 개의 큰 사전 훈련된 모델인 언어 모델(GPT-3)과 텍스트-이미지 모델(Stable Diffusion)의 지식을 결합하여 대규모 이미지 편집 예시 데이터셋을 생성합니다. 우리의 조건부 diffusion 모델인 InstructPix2Pix은 생성된 데이터에 훈련되어 실제 이미지와 사용자가 작성한 지시에 대해서도 일반화되며, 추론 시간에 이미지를 편집합니다. 우리의 모델은 전방향 패스에서 편집을 수행하며, 예시별로 미세 조정이나 inversion을 요구하지 않기 때문에 몇 초 내에 이미지를 빠르게 편집합니다. 다양한 입력 이미지와 서면 지시에 대해 설득력 있는 편집 결과를 보여줍니다.


# 1. Introduction

우리는 이미지 편집을 위한 인간이 작성한 지시를 따를 수 있는 생성 모델을 교육하는 방법을 제시합니다. 이 작업을 위한 훈련 데이터를 대규모로 확보하는 것은 어렵기 때문에, 다양한 모달리티에서 사전 훈련된 여러 대형 모델을 결합한 쌍 데이터셋을 생성하는 접근 방식을 제안합니다: 대형 언어 모델(GPT-3 [7])과 텍스트-이미지 모델(Stable Diffusion [51]). 이 두 모델은 언어와 이미지에 대한 보완적인 지식을 포착하여 두 모달리티를 아우르는 작업을 위한 쌍 훈련 데이터를 생성하는 데 결합될 수 있습니다.

우리가 생성한 쌍 데이터를 사용하여, 입력 이미지와 그것을 편집하는 방법에 대한 텍스트 지시가 주어졌을 때 편집된 이미지를 생성하는 조건부 diffusion 모델을 훈련합니다. 우리의 모델은 전방향 패스에서 직접 이미지 편집을 수행하며, 추가적인 예시 이미지, 입력/출력 이미지의 전체 설명, 또는 예시별 미세 조정을 요구하지 않습니다. 전적으로 합성 예시(즉, 생성된 서면 지시와 생성된 이미지)에 대해 훈련되었음에도 불구하고, 우리의 모델은 임의의 실제 이미지와 자연스러운 인간 작성 지시에 대해 제로-샷 일반화를 달성합니다. 우리의 모델은 객체 교체, 이미지의 스타일 변경, 설정 변경, 예술 매체 변경 등 다양한 편집을 수행할 수 있는 인간의 지시를 따를 수 있는 직관적인 이미지 편집을 가능하게 합니다. 선택된 예시는 그림 1에서 찾을 수 있습니다.

# 2. Prior Work
**Composing large pretrained models** 
최근 연구에서는 대형 사전 훈련된 모델들을 조합하여 단일 모델로는 수행할 수 없는 다모달 작업, 예를 들어 이미지 캡셔닝과 시각적 질문 답변(언어 모델과 텍스트-이미지 모델 모두의 지식이 요구되는 작업)을 해결할 수 있음을 보여주었습니다. 사전 훈련된 모델들을 결합하는 기법에는 새로운 작업에 대한 공동 미세조정[4, 33, 40, 67], 프롬프트를 통한 의사소통[62, 69], 에너지 기반 모델의 확률 분포 조합[11, 37], 다른 모델로부터의 피드백으로 하나의 모델을 안내하는 방법[61], 그리고 반복적 최적화[34]가 있습니다. 우리의 방법은 GPT-3 [7]과 Stable Diffusion [51]이라는 두 개의 사전 훈련된 모델의 보완적 능력을 활용한다는 점에서 이전 작업과 유사하지만, 이 모델들을 사용하여 쌍을 이루는 다모달 훈련 데이터를 생성한다는 점에서 차이가 있습니다.

**Diffusion-based generative models** 
최근 확산 모델에서의 진보[59]는 최첨단 이미지 합성[10, 18, 19, 53, 55, 60] 뿐만 아니라 비디오[21, 58], 오디오[30], 텍스트[35], 네트워크 매개변수[45] 등 다른 모달리티의 생성 모델을 가능하게 했습니다. 최근의 텍스트-이미지 확산 모델들[41, 48, 51, 54]은 임의의 텍스트 캡션에서 사실적인 이미지를 생성할 수 있음을 보여주었습니다.

**Generative models for image editing** 
이미지 편집 모델은 전통적으로 스타일 전송[15, 16]이나 이미지 도메인 간 변환[22, 24, 36, 42, 71]과 같은 단일 편집 작업을 대상으로 했습니다. 수많은 편집 접근 방식은 이미지를 잠재 공간(예: StyleGAN[25, 26])으로 역전[1–3, 12] 또는 인코딩[8, 50, 63]하여 잠재 벡터를 조작함으로써 편집할 수 있습니다. 최근 모델들은 CLIP[47] 임베딩을 사용하여 텍스트를 사용한 이미지 편집을 안내하는 데 활용되었습니다[5,9,14,28,31,41,44,70]. 우리는 이러한 방법 중 하나인 Text2Live[6]와 비교하는데, 이는 CLIP 유사성 목표를 최대화하는 추가 이미지 레이어를 최적화하는 편집 방법입니다.

최근 연구에서는 텍스트-이미지 확산 모델을 이미지 편집에 사용했습니다[5,17,27,38,48]. 일부 텍스트-이미지 모델은 원래 이미지를 편집할 수 있는 능력을 가지고 있습니다(예: DALLE-2는 이미지의 변형을 생성하고, 영역을 인페인팅하며, CLIP 임베딩을 조작할 수 있습니다[48]), 하지만 이러한 모델들을 대상으로 한 편집에 사용하는 것은 간단하지 않습니다. 대부분의 경우 유사한 텍스트 프롬프트가 유사한 이미지를 생성한다는 보장이 없기 때문입니다. 최근 Hertz 등[17]의 연구는 유사한 텍스트 프롬프트에 대해 생성된 이미지를 흡수하는 Prompt-to-Prompt 방법을 통해 이 문제를 다루었습니다. 이 방법은 생성된 이미지에 대해 고립된 편집을 할 수 있도록 합니다. 우리는 이 방법을 훈련 데이터 생성에 사용합니다. 비생성(즉, 실제) 이미지를 편집하기 위해, SDEdit[38]은 사전 훈련된 모델을 사용하여 입력 이미지에 잡음을 추가하고 새로운 대상 프롬프트로 제거합니다. 우리는 SDEdit을 기준선으로 비교합니다. 다른 최근 작업들은 주어진 캡션과 사용자가 그린 마스크를 기반으로 지역적 인페인팅을 수행하거나[5, 48], 소수의 이미지에서 배운 특정 객체나 개념의 새로운 이미지를 생성하거나[13, 52], 단일 이미지를 역전시키고(미세 조정한 후) 새로운 텍스트 설명으로 재생성하여 편집을 수행합니다[27]. 이러한 접근 방식과는 대조적으로, 우리의 모델은 단일 이미지와 그 이미지를 편집하는 방법에 대한 지시(즉, 어떠한 이미지의 전체 설명도 아님)만을 받아 전방향 패스에서 직접 편집을 수행하며, 사용자가 그린 마스크, 추가 이미지, 또는 예시별 역전이나 미세 조정이 필요하지 않습니다.

**Learning to follow instructions**
우리의 방법은 모델에게 수행할 행동을 알려주는 지시를 통해 편집을 가능하게 한다는 점에서 기존의 텍스트 기반 이미지 편집 작업들[6, 13, 17, 27, 38, 52]과 차별화됩니다. 이는 입력/출력 이미지의 텍스트 라벨, 캡션 또는 설명과는 대조적입니다. 편집 지시를 따르는 주요 이점은 사용자가 자연스러운 서면 텍스트로 모델에게 정확히 무엇을 해야 할지 알릴 수 있다는 것입니다. 사용자가 추가 정보를 제공할 필요가 없습니다. 예를 들어, 입력과 출력 이미지 사이에서 일정하게 유지되는 시각적 내용의 예시 이미지나 설명 같은 것들이 필요하지 않습니다. 지시는 표현력이 풍부하고 정확하며 직관적으로 작성할 수 있어, 사용자가 특정 객체나 시각적 속성의 변경을 쉽게 분리할 수 있습니다. 우리의 목표는 언어 작업을 위해 인간의 지시를 더 잘 따르도록 대형 언어 모델을 교육하는 최근의 작업[39, 43, 68]에 영감을 받아 서면 이미지 편집 지시를 따르는 것입니다.

**Training data generation with generative models**
심층 모델은 일반적으로 대량의 훈련 데이터를 요구합니다. 인터넷 데이터 컬렉션은 종종 적합하지만, 감독을 위해 필요한 형태, 예를 들어 특정 모달리티의 쌍 데이터가 존재하지 않을 수 있습니다. 생성 모델이 계속해서 개선됨에 따라, 이들을 하류 작업을 위한 저렴하고 풍부한 훈련 데이터의 원천으로 사용하는 데 대한 관심이 증가하고 있습니다[32, 46, 49, 57, 64, 65]. 이 논문에서는 두 가지 다른 형태의 상용 생성 모델(언어, 텍스트-이미지)을 사용하여 우리의 편집 모델을 위한 훈련 데이터를 생산합니다.

# 3. Method

우리는 지시 기반 이미지 편집을 감독 학습 문제로 취급합니다: (1) 먼저, 편집 전후의 이미지와 텍스트 편집 지시로 구성된 쌍 훈련 데이터셋을 생성합니다(3.1절, 그림 2a-c 참조), 그리고 (2) 이 생성된 데이터셋에 대해 이미지 편집 diffusion 모델을 훈련합니다(3.2절, 그림 2d 참조). 생성된 이미지와 편집 지시로 훈련되었음에도 불구하고, 우리 모델은 임의의 인간이 작성한 지시를 사용하여 실제 이미지를 편집하는 데 일반화할 수 있습니다. 우리 방법의 개요는 그림 2를 참조하십시오.

## 3.1. Generating a Multi-modal Training Dataset
우리는 서로 다른 모달리티에서 작동하는 두 개의 대규모 사전 훈련된 모델—큰 언어 모델[7]과 텍스트-이미지 모델[51]—의 능력을 결합하여, 텍스트 편집 지시와 해당 편집 전후의 이미지를 포함하는 다모달 훈련 데이터셋을 생성합니다. 다음 두 절에서 이 과정의 두 단계를 자세히 설명합니다. 3.1.1절에서는 GPT-3[7]을 미세 조정하여 텍스트 편집 모음을 생성하는 과정을 설명합니다: 이미지를 설명하는 프롬프트가 주어지면, 변경될 내용을 설명하는 텍스트 지시와 그 변경 후 이미지를 설명하는 프롬프트를 생성합니다(그림 2a). 그 다음, 3.1.2절에서는 편집 전후의 두 텍스트 프롬프트를 텍스트-이미지 모델[51]을 사용하여 해당 이미지 쌍으로 변환하는 과정을 설명합니다(그림 2b).

### 3.1.1 Generating Instructions and Paired Captions
우리는 먼저 텍스트 영역에서 작업을 시작하며, 여기서 대규모 언어 모델을 활용하여 이미지 캡션을 입력받고 편집 지시 및 편집 후 결과적인 텍스트 캡션을 생성합니다. 예를 들어, 그림 2a에서 보여주듯이 입력 캡션 "소녀가 말을 타는 사진"이 주어지면, 우리의 언어 모델은 "소녀가 용을 타게 하라"라는 타당한 편집 지시와 적절하게 수정된 출력 캡션 "소녀가 용을 타는 사진"을 생성할 수 있습니다. 텍스트 영역에서 작업함으로써, 우리는 이미지 변화와 텍스트 지시 사이의 일치를 유지하면서 다양하고 방대한 편집 모음을 생성할 수 있습니다.

우리의 모델은 입력 캡션, 편집 지시, 출력 캡션으로 구성된 비교적 작은 인간 작성 데이터셋에 GPT-3를 미세 조정함으로써 훈련됩니다. 미세 조정 데이터셋을 생성하기 위해 LAION-Aesthetics V2 6.5+ [56] 데이터셋에서 700개의 입력 캡션을 샘플링하고 수동으로 지시와 출력 캡션을 작성했습니다. 우리가 작성한 지시와 출력 캡션의 예는 표 1a에서 볼 수 있습니다. 이 데이터를 사용하여 GPT-3 Davinci 모델을 기본 훈련 매개변수를 사용하여 단일 에폭 동안 미세 조정했습니다.

GPT-3의 방대한 지식과 일반화 능력으로 인해, 우리의 미세 조정된 모델은 창의적이면서도 합리적인 지시와 캡션을 생성할 수 있습니다. GPT-3이 생성한 데이터 예는 표 1b에서 볼 수 있습니다. 우리의 데이터셋은 이 훈련된 모델을 사용하여 다수의 편집과 출력 캡션을 생성함으로써 만들어졌으며, 입력 캡션은 LAION-Aesthetics의 실제 이미지 캡션에서 가져옵니다(중복 캡션 또는 중복 이미지 URL을 제외). 우리는 그 크기가 크고, 내용의 다양성(적절한 명사와 대중 문화 참조 포함), 다양한 매체(사진, 그림, 디지털 아트워크) 때문에 LAION 데이터셋을 선택했습니다. LAION의 잠재적인 단점은 매우 잡음이 많고 비합리적이거나 기술적이지 않은 캡션을 다수 포함한다는 것입니다—하지만, 데이터셋 필터링(3.1.2절)과 분류기 없는 지침(3.2.1절)의 조합을 통해 데이터셋의 잡음이 완화되는 것을 발견했습니다. 최종적으로 생성된 지시와 캡션으로 구성된 우리의 말뭉치는 454,445개의 예를 포함합니다.

### 3.1.2 Generating Paired Images from Paired Captions
다음으로, 사전 훈련된 텍스트-이미지 모델을 사용하여 편집 전후를 나타내는 자막 쌍을 이미지 쌍으로 변환합니다. 자막 쌍을 해당 이미지 쌍으로 변환하는 과정에서 한 가지 도전은 텍스트-이미지 모델이 조건부 프롬프트의 아주 사소한 변경에도 이미지 일관성에 대한 보장을 제공하지 않는다는 것입니다. 예를 들어, "고양이의 사진"과 "검은 고양이의 사진"과 같은 매우 유사한 두 프롬프트는 완전히 다른 고양이 이미지를 생성할 수 있습니다. 이는 우리의 목적에 부적합합니다. 우리는 이 쌍 데이터를 사용하여 모델이 이미지를 편집하도록 훈련하는 데 감독으로 사용하려고 합니다(다른 임의의 이미지를 생성하지 않음). 따라서 우리는 Prompt-to-Prompt[17]를 사용합니다. 이는 최근에 개발된 방법으로, 텍스트-이미지 확산 모델에서 여러 생성물이 유사하도록 장려하는 것을 목표로 합니다. 이는 일부 잡음 제거 단계에서 빌려온 교차 주의 가중치를 통해 이루어집니다. 그림 3은 Prompt-to-Prompt 사용 유무에 따른 샘플 이미지를 비교합니다.

이 방법은 생성된 이미지를 동화하는 데 큰 도움이 되지만, 다른 편집은 이미지 공간에서 다른 양의 변경을 요구할 수 있습니다. 예를 들어, 대규모 이미지 구조를 변경하는 큰 크기의 변경(예: 객체 이동, 다른 모양의 객체로 교체)은 생성된 이미지 쌍 간의 유사성이 덜 필요할 수 있습니다. 다행히도, Prompt-to-Prompt는 두 이미지 간의 유사성을 제어할 수 있는 매개변수를 가지고 있습니다: 공유된 주의 가중치를 가진 잡음 제거 단계의 비율 p입니다. 불행히도, 자막과 편집 텍스트만으로 p의 최적값을 식별하는 것은 어렵습니다. 따라서 우리는 각 자막 쌍마다 100개의 이미지 쌍 샘플을 생성하며, 각각 $p \sim \mathcal{U}(0.1, 0.9)$의 무작위 값을 가지고, 이 샘플들을 CLIP 기반 메트릭을 사용하여 필터링합니다: Gal 등[14]에 의해 도입된 CLIP 공간에서의 방향 유사성입니다. 이 메트릭은 두 이미지 간의 변화와 두 이미지 캡션 간의 변화 사이의 일관성을 측정합니다. 이 필터링을 수행함으로써 우리의 이미지 쌍의 다양성과 품질을 최대화할 뿐만 아니라, 우리의 데이터 생성을 Prompt-to-Prompt와 Stable Diffusion의 실패에 더 강건하게 만듭니다.

## 3.2. InstructPix2Pix
우리는 생성된 훈련 데이터를 사용하여 서면 지시에서 이미지를 편집하는 조건부 확산 모델을 훈련합니다. 이 모델은 대규모 텍스트-이미지 잠재 확산 모델인 Stable Diffusion을 기반으로 합니다.

확산 모델[59]은 데이터 샘플을 노이즈가 없는 오토인코더 시퀀스를 통해 생성하는 것을 배웁니다. 이 시퀀스는 데이터 분포의 점수[23]를 추정합니다(밀도가 높은 데이터를 향하는 방향). 잠재 확산[51]은 사전 훈련된 변분 오토인코더[29]의 잠재 공간에서 운영함으로써 확산 모델의 효율성과 품질을 개선합니다. 이 오토인코더는 인코더 $\mathcal{E}$ 와 디코더 $\mathcal{D}$를 가지고 있습니다. 이미지 $x$에 대해, 확산 과정은 인코딩된 잠재 변수 $z = \mathcal{E}(x)$에 노이즈를 추가하여 노이즈가 있는 잠재 변수 $z_t$를 생성합니다. 여기서 노이즈 수준은 시간스텝 $t$에 따라 증가합니다. 우리는 이미지 조건 $c_I$과 텍스트 지시 조건 $c_T$을 주어진 잠재 노이즈 $z_t$에 더해진 노이즈를 예측하는 네트워크 $\theta$를 배웁니다. 우리는 다음 잠재 확산 목표를 최소화합니다:

$L = \mathbb{E}_{(x, c_I, c_T), t \sim \mathcal{N}(0,1), t \in T} \left[ \left\| z - \theta(z_t, t, \mathcal{E}(c_I), c_T) \right\|^2 \right] \quad (1)$

Wang et al. [66]은 이미지 변환 작업에 대해, 특히 쌍 훈련 데이터가 제한적일 때, 대규모 이미지 확산 모델을 미세 조정하는 것이 처음부터 모델을 훈련하는 것보다 뛰어난 성능을 보인다고 보고했습니다. 그러므로 우리는 이미지-텍스트 생성 능력이 방대한 사전 훈련된 Stable Diffusion 체크포인트로부터 우리 모델의 가중치를 초기화함으로써 이를 활용합니다. 이미지 조건을 지원하기 위해, 첫 번째 합성곱 계층에 추가적인 입력 채널을 추가하고 $z_t$와 $\mathcal{E}(c_I)$를 연결합니다. 확산 모델의 모든 사용 가능한 가중치는 사전 훈련된 체크포인트에서 초기화되며, 새롭게 추가된 입력 채널에서 작동하는 가중치는 0으로 초기화됩니다. 원래 캡션을 위해 의도된 같은 텍스트 조건 메커니즘을 재사용하여 대신 텍스트 편집 지시 $c_T$를 입력으로 받습니다. 추가적인 훈련 세부 사항은 보충 자료의 부록 C에 제공됩니다.
### 3.2.1 Classifier-free Guidance for Two Conditionings
분류기 없는 확산 지침[20]은 확산 모델에 의해 생성된 샘플의 품질과 다양성을 희생하여 보다 더 높은 질의 데이터에 확률 질량을 이동시키는 방법입니다. 이는 일반적으로 클래스 조건부 및 텍스트 조건부 이미지 생성에 사용되어 생성된 이미지의 시각적 품질을 향상시키고, 샘플링된 이미지가 그들의 조건부에 더 잘 일치하도록 합니다. 분류기 없는 지침은 암시적인 분류기 $p_\theta (c | z_t)$가 조건부 c에 높은 가능성을 할당하는 곳으로 확률 질량을 효과적으로 이동시킵니다. 분류기 없는 지침의 구현은 조건부 및 비조건부 잡음 제거에 대해 확산 모델을 공동으로 훈련하고, 추론 시 두 점수 추정치를 결합하는 것을 포함합니다. 비조건부 잡음 제거 훈련은 훈련 중 일정 빈도로 조건부 c를 고정된 빈값 $c = \varnothing$으로 설정함으로써 단순히 수행됩니다. 추론 시, 지침 규모 s가 1 이상일 때, 수정된 점수 추정치 $\tilde{e_\theta}(z_t, c)$는 조건부 $e_\theta(z_t, c)$ 방향으로 확대되고 비조건부 $e_\theta(z_t, \varnothing)$에서 멀어집니다.

$$
\tilde{e_\theta}(z_t, c) = e_\theta(z_t, \varnothing) + s \cdot (e_\theta(z_t, c) - e_\theta(z_t, \varnothing)) (2)
$$

우리의 작업에 있어서, 점수 네트워크 $e_\theta(z_t, c_I, c_T)$는 두 가지 조건부, 즉 입력 이미지 $c_I$와 텍스트 지시 $c_T$에 대해 갖습니다. 우리는 두 조건부 모두에 대하여 분류기 없는 지침을 활용하는 것이 유익하다는 것을 발견했습니다. Liu et al. [37]은 조건부 확산 모델이 다양한 조건부 값에서 점수 추정치를 조합할 수 있다는 것을 보여주었습니다. 우리는 두 개의 별개의 조건부 입력으로 우리 모델에 같은 개념을 적용합니다. 훈련 중, 우리는 5%의 예시에서만 $c_I = \varnothing$으로 무작위로 설정하고, 5%의 예시에서만 $c_T = \varnothing$으로, 그리고 또 다른 5%의 예시에서는 $c_I = \varnothing$과 $c_T = \varnothing$ 둘 다를 설정합니다. 따라서 우리 모델은 두 조건부 또는 둘 중 하나의 조건부에 대한 비조건부 잡음 제거를 할 수 있습니다. 우리는 두 가지 지침 규모, $s_I$와 $s_T$를 도입합니다. $s_I$를 증가시키면 입력 이미지와 더 밀접하게 닮은 편집된 이미지가 나오고, $s_T$를 증가시키면 더 강렬한 편집이 나타납니다. 수정된 점수 추정치는 다음과 같습니다:

$$
\begin{align*}
\tilde{e_\theta}(z_t, c_I, c_T) &= e_\theta(z_t, \varnothing, \varnothing) \\
&+ s_I \cdot (e_\theta(z_t, c_I, \varnothing) - e_\theta(z_t, \varnothing, \varnothing))\\
&+ s_T \cdot (e_\theta(z_t, c_I, c_T) - e_\theta(z_t, c_I, \varnothing))
\end{align*}
$$

그림 4에서는 이 두 매개변수가 생성된 샘플에 미치는 영향을 보여줍니다. 분류기 없는 지침 공식에 대한 자세한 사항은 보충 자료의 부록 D에서 확인할 수 있습니다.

# 4. Result

우리는 다양한 종류의 실제 사진과 예술 작품에 대해 많은 편집 유형과 지시어의 표현을 사용한 지시 기반 이미지 편집 결과를 보여줍니다. 선택된 결과는 보충 자료의 그림 1, 5, 6, 7, 11, 12 및 부록 A에서 확인할 수 있습니다. 우리 모델은 객체 교체, 계절 및 날씨 변경, 배경 교체, 재질 속성 수정, 예술 매체 변환 등 다양한 어려운 편집 작업을 성공적으로 수행합니다.

우리는 최근의 작업인 SDEdit [38], Text2Live [6], 그리고 Prompt-to-Prompt [17]와 질적으로 비교합니다. 우리 모델은 이미지를 어떻게 편집할지에 대한 지시를 따르지만, 이전 작업들(이 기준선 방법을 포함)은 이미지(또는 편집 레이어)의 설명을 기대합니다. 따라서, 우리는 편집 지시 대신 편집 후 텍스트 캡션을 제공합니다. 또한, 이미지 일관성과 편집 품질을 측정하는 두 가지 메트릭을 사용하여 SDEdit 및 Prompt-to-Prompt와 정량적으로 비교합니다. 이는 섹션 4.1에서 더 자세히 설명되어 있습니다. 마지막으로, 생성된 훈련 데이터의 크기와 품질이 우리 모델의 성능에 어떤 영향을 미치는지에 대한 연구를 섹션 4.2에서 보여줍니다.

